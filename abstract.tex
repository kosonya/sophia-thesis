\begin{abstract}

Numerous widely used machine learning algorithms work by means of creating
hidden internal representations of the input data. These representations are
typically not a part of the loss function, and thus are learned with no direct
supervision and semantics attached to them. The algorithms that use such
representations include matrix factorization for collaborative filtering
recommender systems, which learn latent factors encoding the properties of items
being recommended and the preferences of users, and neural networks, which learn
higher-level conceptual representations of input features in intermediate
layers.

The existence of such hidden representations lacking obvious semantics poses a
challenge for accountability and fairness of such models. The model may
inadvertently infer and represent sensitive attributes (such as gender or race,
even if they are not present as input features but can be reasonably accurately
predicted from them) and use them to compute the final output with neither users
nor maintainers of the machine learning system being aware of it. For instance,
depending on circumstances, it may or may not be problematic or even illegal for
a credit decision system to have disparate impact on race, but it's definitely
problematic from the fairness perspective for it to have inferred race as its
latent factor (e.g. from the ZIP Code and income) and use it in the final
decision. Such inferences need to be identified in order to enhance transparency
and mitigated.

This dissertation tackles this problem. We provide one result so far and
identify directions for further exploration. In our first result, we create
explanations of movie recommendations in terms of externally available movie
metadata which are demonstrably reflective of the users' preferences. We use
matrix factorization to create a movie recommendation system based on MovieLens
data set. Then present Latent Factor Interpretation (LFI), a method for
interpreting models by leveraging interpretations of latent factors in terms
of human-understandable features. The interpretation of latent factors can then
replace the uninterpreted latent factors, resulting in a new model that
expresses predictions in terms of interpretable features. Then the
interpretable features are analyzed using quantitative input influence (QII)
method. This is used to generate explanations for given recommendations, which
can then be compared with known user preferences to evaluate how correct they
are. 

In our second ongoing work, we are using a publicly available data set of home
mortgage decisions in order to investigate potentially problematic use of
sensitive attributes in such loan decisions. In this case, a classification
algorithm can be used to make the allow/deny decisions based on a number of
demographic and financial features of applicants. While the use of sensitive
attributes such as gender is generally not allowed, certain features such as
income may correlate with it, and their use can be justified as a business
necessity. Therefore, since income may convey information about gender, it is
undesirable to force the system to use a representation that completely lacks
information about it. Instead, the goal it to create a representation that
contains information about gender only in as much as it is represented by
features whose use is explicitly justified as necessary. The goal of this work
is to create a system capable of removing the use of protected attributes
subject to such business necessity constraints.

One direction of the proposed future work is apply the methods for analyzing
recommendation systems to the online class recommendation data set from
Coursera. The reasons for recommending various classes to a user can be then
analyzed and used to provide more structure to recommendations, such as by
grouping them into categories being recommended for different sets of reasons.
Furthermore, this analysis can be used to ensure fairness in recommendations
and offset pre-existing biases in the data set, such as if it turns out that
there is a noticeable gender disparity in the courses being taken and it is
being perpetuated by biased recommendations.

Another step being proposed is the development of methods of partial
explanation and isolating uninterpretability. Existing interpretation methods
analyze models as-is. This poses a challenge because they're likely to involve
distributed representations where interpretable pieces of information (or at
least interpretable with available data) are intertwined with uninterpretable
ones. This may lead to inaccurate or confusing explanations that are attempting
to approximate the true semantics of the representation with available
information regardless of how well that can be done. Instead of that, the model
can be transformed while preserving its behavior such that uninterpretable parts
of the representation are isolated, correctly labeled as such, and the rest of
the representation is explained with a higher precision. The goal of this work
would be to develop and test such methods.


\end{abstract}
