\section{Conditional debiasing}

In many situations, interpretation alone may not be enough, and the model may
have to be changed in a way that removes bias from them. Many studies focus on
removing the use of sensitive attributes entirely\cite{Beutel2017DataDA, TschantzDW12}.
This may not be desirable in situations where
some use or the use of features correlated with sensitive ones is justifiable.

We propose a method for removing the non-justified use of protected attributes or
their correlates from machine learning models. This method is applicable to models that have an
internal numeric representation of the input data which are trained with
gradient-based methods. Such models include neural networks, matrix
factorization models, and linear models.

Generally, the model transforms a set of input features $X$ into a single target
variable $Y$. $X$ can be divided into the following subsets:

\begin{itemize}
	\item
		$X_{bn}$ -- business necessity features, that the model is
		allowed to use in any capacity (note that the data set may not
		contain a feature with this property, in which case it should be
		constructed during preprocessing; for example, if it’s
		established that airlines are justified in requiring pilots to
		be at least 5'5" tall but not justified in posting stricter
		height requirements, then a business necessity feature would be
		a binary ``is the applicant at least 5'5" tall'' rather than
		numeric height);
	\item
		$X_{pr}$ -- protected features (such as gender or race), that
		the decisions of the model may not depend on and may not create
		disparate impact on to an extent larger than explicitly 
		justified by business necessity features. In this situation, the
		use of a protected feature is said to be mediated by the
		business necessity features;
	\item
		$X_{gen}$ - other features, that may be used in decisions so
		long as they don’t create disparate impact on protected features.
\end{itemize}

The use of protected features can be mitigated either at the level of the model
output $Ŷ$ or internal feature representation R, which can be an inner layer of a
neural network. After the level at which the use is to be removed is selected,
it can be denoted as $V$. The desired property of a model -- not using protected
features or their correlates to an extent greater than business necessity -- can
be formally defined as $V\perp X_{pr}|X_{bn}$.

Since conditional independence in a binary property, it is difficult to optimize
for, so instead we replace it with an equivalent definition based on conditional
mutual information: $I(V;X_{pr}|X_{bn}) = 0$. Then, the goal can be formulated
as minimizing the value of this term, and it can be added to the model’s loss
function $L$, which the training process minimizes with gradient descent:
$L' = L + I(V;X_{pr}|X_{bn})$.

By definition,

\begin{equation}
I(V;X_{pr}|X_{bn}) = \sum_{v \in V} \sum_{x_{pr} \in X_{pr}}
\sum_{x_{bn} \in X_{bn}} p_{V,X_{pr},X_{bn}}(v,x_{pr},x_{bn}) \cdot log
\frac{p_{X_{bn}}(x_{bn})\cdot p_{V,X_{pr},X_{bn}}(v,x_{pr},x_{bn})}
{p_{V,X_{bn}}(v,x_{bn}) \cdot p_{X_{pr},X_{bn}}(x_{pr},x_{bn})}
\end{equation}

Thus, computing $I(V;X_{pr}|X_{bn})$ requires estimating several probability
distributions. This poses several challenges. First, each of $V$, $X_{pr}$,
$X_{bn}$ can itself be highly-dimensional, making the problem challenging by
the fact that data examples are unlikely to cover the entirety of the possible
space. Second, while $p_{X_{bn}}(x_{bn})$ and $p_{X_{pr},X_{bn}}(x_{pr},x_{bn})$
only depend on the input data, and thus can be precomputed before the step of
model optimization, all other distributions in this equation depend on $V$,
which changes depending on the parameters of the model. Thus, in order for the
model to be able to optimize for the loss function $L' = L + I(V;X_{pr}|X_{bn})$,
the entirety of the process of density estimation must be differentiable so that
gradient descent could be used for optimization.

While conditional mutual information term that we are adding can look similar to
cross entropy loss, which is commonly used in neural network training, and the
differentiation of which is a solved problem, the ability to differentiate it
hinges on treating the output of the output layer and the one-hot encoding of
the training data as probability distributions. In other words, the existing
solutions treat distributions as given and only compute the cross-entropy term
based on them. In our case, the distribution is not given; in fact, it cannot be
even computed on a single training example (which is what the standard
cross-entropy loss term does), but rather is an aggregate statistic of an entire
training minibatch (which also puts an additional constraint on using stochastic
gradient descent for training: the minibatch has be to large enough for the
density estimation to be accurate). This statistic has be computed in the run time,
and in an analytically differentiable way so that gradient of the entire loss
function could be computed as well.

The ability to compute the gradient is desirable because gradient-based
optimization is very commonly used in deep learning\cite{Goodfellow-et-al-2016},
and therefore it is easier to incorporate into existing systems and models. At
this point, however, it is unclear to us whether accurate and differentiable
density estimation can be done for the case general enough to be applicable to a
large enough set of real-life problems. If it turns out to be impossible, the
alternative strategy would involve computing density in the most applicable way
without the constraint of differentiability, and then using either numeric
gradient estimation (which would be slower but in this case necessary) or using
non-gradient-based optimization techniques.

So far, our research into this problem involved using Home Mortgage Disclosure
Act data set\cite{data-hmda}, for which we were
using predictive models using TensorFlow framework. This framework offer a model
of ``end-to-end differentiable'' computation, which means that if we implement a
computation using the tools that TensorFlow provides, the computation is
guaranteed to be differentiable, and TensorFlow can automatically compute the
gradient and run optimization on it.

TensorFlow offers implementation of statistical analysis methods
which can be used in kernel density estimation and are differentiable
\cite{171110604}. However, in order to maintain the desired properties, this
implementation has significant constraints, such as not supporting the addition
of distributions. We are yet to determine whether this allows us to compute the
mutual information term or we have to use another method. Furthermore, while the
implementation of kernel density estimation is already available, we need to
determine whether this method is applicable in our task or some other method of
density estimation would be a better fit.

The proposed work in this section is to address these challenges in estimating
distributions -- possibly by imposing certain constraints on the kinds of models
and data sets that can this method can be applicable to -- and implement a
system capable of training debiased models subject to constraint of maintaining
business necessity features.

So far, the complexity of the density estimation problem and the complexity of
the HMDA data set have prevented us from testing a broader hypothesis that the
method of minimizing conditional mutual information itself is viable. In order to
isolate the complexity of this problem, we are planning to generate series of
smaller-scale synthetic data sets, which have the lowest possible dimensionality
for the method to be applicable, and for which we can know the distributions and
expected outcomes exactly. Specifically, if we only use binary features or
discrete features with relatively few categories, use also binary or
discrete output $Ŷ$ for optimization, and make sure that the data set is dense
enough in the feature space, density estimation can be solved by simple counting,
in which case we don't need to solve the problem of density estimation in a 
more general case. Thus, we can test the overall method and possibly test
non-gradient optimization techniques as well, and then check how far we can
generalize from the simplest possible case.
