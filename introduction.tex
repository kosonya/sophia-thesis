\chapter{Introduction}

\section{Motivation}

The widespread adoption of machine learning systems that have opaque
decision-making mechanisms has lead to a growing number of privacy and fairness
concerns.

Most notably, recommender systems based on matrix factorization
input only the information about the behavior of users with regard to items
being recommended (e.g. movies on a movie rating website, products in an
online store) and from that they infer certain properties of the items and
user preferences in terms of these properties\cite{facebook-cf}. The properties
being inferred are unknown both to the system and to its users. Sometimes,
developers can eyeball these properties and conclude that, for example, a movie
recommender system, among other properties, infers whether a movie is more
female-oriented or male-oriented and whether it's more serious or escapist.
Correspondingly, it also infers whether each user prefers more ``masculine'' or
``feminine'' and more ``serious'' or ``escapist'' movies\cite{koren2009matrix}.
Unfortunately, such interpretations cannot always be automatically provided.
Automatically provided explanations are often expressed in terms of user or
item similarity (i.e. ``we recommend you item X because you liked items Y and
Z'') or even building a graph of user similarity\cite{hernando2013trees}
without giving insight into the internal inferences being made (such as
``we recommend you item X because we think that X is masculine and you like
masculine things''). While the gender-based targeting may or may not be seen as
problematic in the context of movie recommendations, but it is very likely to be
deemed problematic in the context of job recommendations, given the previous
history of controversies surrounding gendered targeting of job ads
\cite{datta_tschantz_datta_2015}.

Likewise, artificial neural networks can be used in credit decisions
\cite{WEST20001131}, predictive policing\cite{Seo2018PartiallyGN}, and other
applications where it is crucial (and often legally mandatory) to ensure
fairness. In practice, however, these networks can infer and use in their
decisions sensitive attributes such as gender even when they are not present
as features in the training data set\cite{Beutel2017DataDA}.

The issue is further complicated by the fact that the mere absence of usage of
sensitive protected attributes is not always sufficient to maintain legality:
if a certain selection criterion creates disparate impact on a sensitive
attribute (i.e. members of a protected group are being disproportionately
affected by it), it can be deemed illegal unless it can be justified as business
necessity. Furthermore, this can be specific to a particular predicate on a
feature rather than the feature itself: for example, it can be legal for an
airline to require candidates for pilot jobs to be at least 5'2" tall because
that is necessary to reach all of the aircraft controls but it is illegal to
set the minimum requirement at a higher point of 5'6" because that
disproportionately selects against women and is not justified as necessary
for a pilot\cite{RoseMaryCourtCase}.

\section{Completed work}

In our study of interpreting latent factors of recommender systems, we introduce
a notion of a shadow model -- an interpretable model that mimics the behavior
of the original uninterpretable model as closely as possible. We are trying to
preserve both the external behavior and the internal structure of the original
model, and we show that these goals are not always equally achievable at the
same time. Specifically, we work with a MovieLens data set to create a
movie recommendation system based on matrix factorization. We then collect
metadata about movies present in the dataset from publicly available sources
such as IMDB and TV Tropes. We use that metadata to predict the values of latent
factors representing the properties of the movies. Then, we analyze these
predictions with quantitative input influence (QII) method
\cite{datta2016algorithmic} and use this analysis to infer the meaning of the
factors in the original model. This inference is further verified in experiments
with simulated users, for whom the true movie preferences are known, and thus
we can compare how accurately each explanation corresponds to properties the
users actually care about.

\section{Proposed work}

A substantial limitation of the method outlined above is that not all
item properties inferred by the matrix factorization system can be interpreted.
This can be sometimes a limitation of the available metadata: for example,
if the data set contains tags about whether movies are oriented towards male or
female audience, the shadow model would be able to capture that meaning of a
latent factor, but if there are no tags about the movies being ``serious'' or
``escapist'', the shadow model would do its best at predicting its values from
other available data, which may result in overfitting or completely
unintelligible explanations. In practice, we have observed that most latent
factors tend to be predicted with roughly the same accuracy, which means that
all of them contain a mix of interpretable and uninterpretable information. The
proposal is to fix that by transforming the model in a way that isolates
uninterpretable factors, which can be left as ``black boxes'' in the
explanation, while improving the predictability from metadata (and thus
interpretability) of other factors. This can be done, for example, by first
training the model, and then optimizing it for this kind of interpretability
subject to constraint that the output must remain unchanged. Alternatively, it
may be done by creating an invertible transformation matrix that can be applied
before and after generating explanations. The crucial contribution that we
expect to make here is the formalization of the isolated interpretability loss
function as well as development of a method for optimizing for it.

In addition, we hope to collaborate with Coursera to make a case study of
transparency and explanations in course recommendations, where it is
particularly important to understand why certain courses are being recommended.

\section{Ongoing work}

Transparency alone is sufficient in some cases, but in others it is also
important to modify a model in a way that enforces fairness if it is violated.
There exist methods for completely removing information about sensitive
attributes from a model's internal representations\cite{Beutel2017DataDA}, but
this is not always applicable. For example, if a system is making decisions
about loans, it should not rely on gender, but it can be justifiable for it to
rely on applicants' income. However, income can be correlated with gender, in
which case ensuring complete unpredictability of gender from the internal
representation will also remove or distort information about income, which is
undesirable. Thus, we are developing a method for removing the information
about sensitive attributes subject to constraint that the information about
features justifiable as business necessity must remain. The main challenge here
is that doing such optimization requires estimating the distribution of the
internal representation in a way that is differentiable (so that it would be
possible to optimize on this metric) and makes as few assumptions about this
distribution as possible.

While the main focus of this thesis is on matrix factorization models, this
method is also applicable to neural networks and linear models. One advantage of
applying it to neural networks is that they can solve classification and not
just recommendation problems, and it tends to be more feasible to obtain data
sets for classification problems that have sensitive attributes available. For
example, we have so far been working with HMDA loan decisions data set and a
neural network that predicts them. The application of this method to neural
networks poses an additional challenge of selection at which point the
intervention should be made: an intermediate layer or the output. In matrix
factorization models, the most natural choice is the intermediate layer, and in
linear models the only possible choice is the output.

\section{Thesis statement}

\emph{Representation aware accounting for matrix factorization models enables
explanations for for recommendations and mitigates bias.}

\section{Related work}

As was mentioned above, multiple studies exist that have been exploring the
area of this thesis.

\paragraph{Latent factor interpretation}
The prediction of latent factor based on metadata has been
previously tried\cite{gantner2010learning}, but not in the context of creating
explanations and not with the goal of preserving causal correspondence between
the original model and the shadow model. Some attempts to interpret latent
factors rely on the authors looking at the data and making a judgment call
without relying on a formal methodology that can be generalized
\cite{koren2009matrix}. Others use topic modeling, which results in features
that may not be necessarily understandable to the users
\cite{rossetti2013towards}. The technique of training an approximate, but
interpretable shadow
model for LFI is similar in spirit to approaches to explaining other
machine learning
systems~\cite{craven1995extracting,ribeiro2016lime,sanchez2015towards}.
An important difference is that prior work has explored this idea
using the features present in the task itself, or using pre-defined
mappings to an interpretable space. 
Instead, we use externally available interpretable features and
\emph{learn} the mapping to an interpretable space.
We also differ from existing approaches that attribute meaning to
latent factors, e.g.
with topic models\cite{rossetti2013towards}, in that the constructed
shadow model is itself a recommendation model, albeit with
interpretable inputs, and is therefore amenable to existing
explanation techniques for machine learning models.


\paragraph{Conditional debiasing} The existing research on removing the use of
sensitive attributes from
representations focuses on removing them completely, without constraining the
removal by business necessity, which substantially limits its applicability in
real world scenarios\cite{Beutel2017DataDA}, or makes strong assumptions about
the underlying distributions\cite{Kamishima}. Previous work on proxy use
\cite{Datta:2017:UPD:3133956.3134097} solves a closely related problem of removing
the use of correlates of sensitive attributes (``proxies'') from models, but it
assumes a model in which no distributed representations exist, and proxies are
limited to a small number of subcomputations rather than spread out across the
entire internal representation. A lot of studies have been done on restricting
the explicit use of protected information\cite{TschantzDW12} but fewer deal with
indirect inferences.

